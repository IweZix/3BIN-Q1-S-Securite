{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ipl I317B Sécurité : labos\n",
    "## Semaine 1 : introduction et outils python\n",
    "\n",
    "### Partie 1 : manipuler du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1\n",
    "Pour ce premier exercice, nous allons manipuler un court extrait de logs d'un serveur web nginx.  \n",
    "Pour chaque ligne de celui-ci, extrayez:\n",
    "- l'adresse IP\n",
    "- le timestamp\n",
    "- le chemin demandé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = \"\"\"\n",
    "    91.160.48.68 - - [3/Sep/2020:18:18:10 +0200] \"GET /?C=M;O=D HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\" \"-\"\n",
    "    78.224.72.158 - - [3/Sep/2020:18:24:52 +0200] \"GET / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\" \"-\"\n",
    "    192.241.199.4 - - [3/Sep/2020:19:11:15 +0200] \"GET / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 zgrab/0.x\" \"-\"\n",
    "    40.121.69.161 - - [3/Sep/2020:19:16:32 +0200] \"GET /.env HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" \"-\"\n",
    "    40.121.69.161 - - [3/Sep/2020:19:16:32 +0200] \"POST / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" \"-\"\n",
    "    185.172.110.223 - - [3/Sep/2020:19:16:37 +0200] \"GET / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:76.0) Gecko/20100101 Firefox/76.0\" \"-\"\n",
    "    66.249.75.16 - - [3/Sep/2020:19:23:28 +0200] \"GET / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.97 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\" \"-\"\n",
    "    66.249.75.23 - - [3/Sep/2020:19:23:30 +0200] \"GET / HTTP/1.1\" 200 0 \"-\" \"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP:  - Timestamp: 3/Sep/2020:18:18:10 +0200 - Path: /?C=M;O=D\n",
      "IP:  - Timestamp: 3/Sep/2020:18:24:52 +0200 - Path: /\n",
      "IP:  - Timestamp: 3/Sep/2020:19:11:15 +0200 - Path: /\n",
      "IP:  - Timestamp: 3/Sep/2020:19:16:32 +0200 - Path: /.env\n",
      "IP:  - Timestamp: 3/Sep/2020:19:16:32 +0200 - Path: /\n",
      "IP:  - Timestamp: 3/Sep/2020:19:16:37 +0200 - Path: /\n",
      "IP:  - Timestamp: 3/Sep/2020:19:23:28 +0200 - Path: /\n",
      "IP:  - Timestamp: 3/Sep/2020:19:23:30 +0200 - Path: /\n"
     ]
    }
   ],
   "source": [
    "for line in logs.split(\"\\n\"):\n",
    "      if line:\n",
    "            print(\"IP:\", line.split(\" \")[0],\n",
    "                    \"- Timestamp:\", line.split(\"[\")[1].split(\"]\")[0],\n",
    "                    \"- Path:\", line.split(\"\\\"\")[1].split(\" \")[1]\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 2\n",
    "Avec les mêmes logs que l'exercice précédent, comptez et affichez les différents user-agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 : Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\n",
      "1 : Mozilla/5.0 zgrab/0.x\n",
      "2 : Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\n",
      "1 : Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:76.0) Gecko/20100101 Firefox/76.0\n",
      "1 : Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.97 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\n",
      "1 : \n"
     ]
    }
   ],
   "source": [
    "liste = {}\n",
    "\n",
    "for line in logs.split(\"\\n\"):\n",
    "    if line:\n",
    "        userAgent = line.split(\"\\\"\")[5]\n",
    "        \n",
    "        if userAgent not in liste:\n",
    "            liste[userAgent] = 1\n",
    "        else:\n",
    "            liste[userAgent] += 1\n",
    "\n",
    "for key, value in liste.items():\n",
    "    print(value, \":\", key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 2 : requests et BeautifulSoup\n",
    "\n",
    "Assurez vous que les paquets suivant soient bien installé :\n",
    "\n",
    "`pip3 install --user requests bs4`\n",
    "\n",
    "`sudo pip3 install requests bs4`\n",
    "\n",
    "Depuis un jupyter notebook, vous pouvez probablement juste executer la case suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install --user requests bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le packet requests nous permet de faire des requêtes web facilement, par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "ipl_req = requests.get(\"https://www.vinci.be/fr/formations/informatique-developpement-applications\")\n",
    "\n",
    "if ipl_req.ok:\n",
    "    print(\"ok\")\n",
    "    ipl_html = ipl_req.text\n",
    "else:\n",
    "    print(ipl_req.status_code)\n",
    "    print(ipl_req.reason)\n",
    "\n",
    "# Doc requests : https://requests.readthedocs.io/en/latest/ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Nous pouvons ensuite lire le code source de la page en affichant `ipl_html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n    <!doctype html>\\n<html class=\"no-js preload\" lang=\"fr\">\\n<head>\\n    \\n<!-- Biskit SDK Script -->\\n<script>\\nwindow.biskitSdk = window.biskitSdk || {}\\nwindow.biskitSdk.consentFunction = null\\nwindow.biskitSdk.registerConsentFunction = (callback) => {\\n    window.biskitSdk.consentFunction = callback\\n    return true\\n}\\n</script>\\n<!-- END Biskit SDK Script -->\\n\\n<!-- Google Tag Manager -->\\n<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\'gtm.start\\':\\nnew Date().getTime(),event:\\'gtm.js\\'});var f=d'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipl_html[:500]  # 500 premier caractères"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Parser du html avec des split ou des regex deviendrait vite infernal, c'est là qu'intervient la librairie BeautifulSoup qui nous permet de parcourir beaucoup plus simplement le dom notamment avec les fonctions `find` et `find_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informatique - développement d’applications | HE VINCI\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(ipl_html)\n",
    "\n",
    "# pour récupérer le contenu de la balise <title> :\n",
    "title = soup.find(\"title\").get_text()\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `find` et la fonction `find_all` vous renverons un object de type `tag` (balise) sur lequel vous pouvez faire :\n",
    "  * d'autres `find()` et `find_all()`\n",
    "  * `get_text()` pour récupérer tout le contenu textuel de la balise\n",
    "  * `[\"nom de l'attribut\"]` pour récupérer un attribut en particulier (exemple: `tag['href']`)\n",
    "  * ...\n",
    "\n",
    "( Doc BeautifulSoup4 : https://www.crummy.com/software/BeautifulSoup/bs4/doc/ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1:\n",
    "Récupérez le contenue de la page suivante et affichez son titre principal (la balise \"h1\") :https://www.vinci.be/fr/formations/informatique-developpement-applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Informatique - développement d’applications\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "ipl_req = requests.get(\"https://www.vinci.be/fr/formations/informatique-developpement-applications\")\n",
    "\n",
    "if ipl_req.ok:\n",
    "    ipl_html = ipl_req.text\n",
    "\n",
    "    soup = BeautifulSoup(ipl_html)\n",
    "\n",
    "    title = soup.find(\"h1\").get_text()\n",
    "\n",
    "    print(title)\n",
    "else:\n",
    "    print(ipl_req.status_code)\n",
    "    print(ipl_req.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup.find_all(\"a\", href=\"https://www.facebook.com/HEVINCI/\")Il est possible de faire des requêtes beaucoup plus poussée en ajoutant des arguments correspondant aux attributs des balises que nous cherchons, par exemple pour trouver les balises `<div class=\"footer\"></div>` :\n",
    "\n",
    "```\n",
    "soup.find_all(\"div\", class_=\"footer\")\n",
    "# NB: class est un mot clé réservé en python, d'où l'ajout du \"_\" dans BeautifulSoup\n",
    "\n",
    "# BeautifulSoup accepte également la notation suivante où class n'est plus un mot clé mais un string :\n",
    "soup.find_all(\"div\", {\"class\": \"footer\"})\n",
    "```\n",
    "\n",
    "Ou encore pour trouver tous les liens vers la page facebook de l'ipl :\n",
    "\n",
    "`soup.find_all(\"a\", href=\"https://www.facebook.com/HEVINCI/\")`\n",
    "\n",
    "BeautifoulSoup accepte également des regex compilée en argument, il est donc possible de récupérer la liste des url qui ne sont pas du domaines de \"vinci.be\" :\n",
    "\n",
    "```\n",
    "import re\n",
    "\n",
    "not_vinci = re.compile(\"^http(s?)://(?!www.vinci.be/).*$\")\n",
    "soup.find_all(\"a\", href=not_vinci)\n",
    "```\n",
    "\n",
    "Il est également possible d'utiliser une fonction arbitraire pour valider les liens. Par exemple, pour trouver tous les liens de moins de 30 caractères :\n",
    "\n",
    "```\n",
    "def less_than_30(tag):\n",
    "    return len(tag) < 30  # on retourne un booleen\n",
    "    \n",
    "# on passe notre fonction en paramètre sans l'executer :\n",
    "soup.find_all(\"a\", href=less_than_30)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 2:\n",
    "\n",
    "Utilisez des regex pour récupérer la liste des liens http (et donc pas https) sur la page :  \n",
    "https://www.vinci.be/fr/formations/informatique-developpement-applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.enseignement.be/index.php?page=4323\" rel=\"noreferrer noopener\" target=\"_blank\">enseignement.be</a>]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "http = re.compile(\"^http://*\")\n",
    "\n",
    "ipl_req = requests.get(\"https://www.vinci.be/fr/formations/informatique-developpement-applications\")\n",
    "\n",
    "if ipl_req.ok:\n",
    "    ipl_html = ipl_req.text\n",
    "\n",
    "    soup = BeautifulSoup(ipl_html)\n",
    "\n",
    "    title = soup.findAll(\"a\", href=http)\n",
    "\n",
    "    print(title)\n",
    "else:\n",
    "    print(ipl_req.status_code)\n",
    "    print(ipl_req.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 3:\n",
    "\n",
    "Utilisez une fonction pour récupérer tous les liens qui finissent avec une extension désignant un type de fichier (ex: \".html\", \".php\", \".pdf\", ...) sur la page :  \n",
    "https://www.vinci.be/fr/formations/informatique-developpement-applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.vinci.be/uploads/brochures/INFORMATIQUE_HEVINCI_23-24.pdf\n",
      "https://progcours.vinci.be/cocoon/programmes/P1INFV01_C.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_links_with_extensions(url, extensions):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    pattern = re.compile(r\".*\\.(\" + \"|\".join(extensions) + r\")$\")\n",
    "    \n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    filtered_links = [link['href'] for link in links if pattern.match(link['href'])]\n",
    "    \n",
    "    return filtered_links\n",
    "\n",
    "url = \"https://www.vinci.be/fr/formations/informatique-developpement-applications\"\n",
    "extensions = [\"html\", \"php\", \"pdf\"]\n",
    "filtered_links = get_links_with_extensions(url, extensions)\n",
    "\n",
    "for link in filtered_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 4:\n",
    "Depuis le site du Centre Cybersecurité Belgique https://ccb.belgium.be/fr/actualit%C3%A9s , récupérez la liste des articles récents et affichez leurs titres ainsi qu'un lien vers l'article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Belgique parmi les meilleurs au monde pour la cybersécurité \n",
      "  ↳ https://ccb.belgium.be/fr/actualit%C3%A9/la-belgique-parmi-les-meilleurs-au-monde-pour-la-cybers%C3%A9curit%C3%A9\n",
      "Evénement hybride SANS et CCB sur la sécurité du cloud \n",
      "  ↳ https://ccb.belgium.be/fr/actualit%C3%A9/ev%C3%A9nement-hybride-sans-et-ccb-sur-la-s%C3%A9curit%C3%A9-du-cloud\n",
      "Venez nous rendre visite à BruCON 2024 \n",
      "  ↳ https://ccb.belgium.be/fr/actualit%C3%A9/venez-nous-rendre-visite-%C3%A0-brucon-2024\n",
      "Mesures administratives et amendes sous NIS2 \n",
      "  ↳ https://ccb.belgium.be/fr/actualit%C3%A9/mesures-administratives-et-amendes-sous-nis2-0\n",
      "Graves problèmes informatiques dans des entreprises belges qui appliquent la mise à jour de CrowdStrike \n",
      "  ↳ https://ccb.belgium.be/fr/actualit%C3%A9/graves-probl%C3%A8mes-informatiques-dans-des-entreprises-belges-qui-appliquent-la-mise-%C3%A0-jour\n",
      "La notification des incidents NIS2 \n",
      "  ↳ https://ccb.belgium.be/fr/actualit%C3%A9/la-notification-des-incidents-nis2\n",
      "Les mesures de cybersécurité à adopter dans le cadre de NIS2 \n",
      "  ↳ https://ccb.belgium.be/fr/actualit%C3%A9/les-mesures-de-cybers%C3%A9curit%C3%A9-%C3%A0-adopter-dans-le-cadre-de-nis2\n",
      "Prochains événements sur le financement de la cybersécurité par l'UE  \n",
      "  ↳ https://ccb.belgium.be/fr/actualit%C3%A9/prochains-%C3%A9v%C3%A9nements-sur-le-financement-de-la-cybers%C3%A9curit%C3%A9-par-lue\n",
      "Prêts pour NIS2 ? \n",
      "  ↳ https://ccb.belgium.be/fr/actualit%C3%A9/pr%C3%AAts-pour-nis2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://ccb.belgium.be/fr/actualités\"\n",
    "\n",
    "ipl_req = requests.get(url)\n",
    "\n",
    "if ipl_req.ok:\n",
    "    ipl_html = ipl_req.text\n",
    "\n",
    "    soup = BeautifulSoup(ipl_html)\n",
    "\n",
    "    title = soup.findAll(\"h2\")\n",
    "    articles = soup.findAll(\"article\")\n",
    "\n",
    "    for i in range(len(title)):\n",
    "        print(title[i].get_text(), \"\\n  ↳ https://ccb.belgium.be\" + articles[i].get(\"onclick\").split(\"'\")[1])\n",
    "\n",
    "else:\n",
    "    print(ipl_req.status_code)\n",
    "    print(ipl_req.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 5:\n",
    "La lib requests permet également d'effectuer de requête POST. Pour tester cette fonctionnalitée, créez un \"httpDump\" sur le site https://httpdump.app/ puis utilisez requests pour poster des données vers votre requestbin.\n",
    "\n",
    "> httpdump\n",
    "> This free service gives you a unique URL to send requests to and inspect them. \n",
    "\n",
    "\n",
    "Voir : https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request (ou la cheatsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "python-requests/2.32.2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://httpdump.app/dumps/89bc59bc-92ee-4a7d-8298-c18112b83598\"\n",
    "\n",
    "# post request\n",
    "req = requests.post(url, data={\"usename\": \"IweZix\", \"password\": \"MySuperPassword\"})\n",
    "\n",
    "if req.ok:\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    print(req.status_code)\n",
    "    print(req.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 6:\n",
    "Vous avez certainement constaté sur votre requestbin que la librairie requests utilise par défaut un User-Agent explicite \"*je suis un lib python pour faire des requêtes*\".  \n",
    "Documentez-vous sur la manière de changer celui-ci dans requests et remplacez le dans votre précédente requête vers httpdump par quelque chose de plus discret, par exemple l'user-agent de la dernière version de chrome :  \n",
    "`Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36`\n",
    "\n",
    "source: https://www.whatismybrowser.com/guides/the-latest-user-agent/chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://httpdump.app/dumps/89bc59bc-92ee-4a7d-8298-c18112b83598\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# post request\n",
    "req = requests.post(url, data={\"usename\": \"IweZix\", \"password\": \"MySuperPassword\"}, headers=headers)\n",
    "\n",
    "if req.ok:\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    print(req.status_code)\n",
    "    print(req.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3 (bonus):\n",
    "\n",
    "Note: dans les jupyter notebook, vous pouvez exécuter des commandes shell en préfixant une cellule d'un `!`. Par exemple : `!date`.\n",
    "\n",
    "#### Exercice 1\n",
    "\n",
    "Pour ce premier exercice, nous allons manipuler un court extrait de logs \"logs.txt\" d'un serveur web nginx (le même que pour la partie 1).\n",
    "\n",
    "Pour chaque ligne de celui-ci, extrayez:\n",
    "\n",
    "    l'adresse IP\n",
    "    le timestamp\n",
    "    le chemin demandé\n",
    "\n",
    "En n'utilisant que des commandes bash : cat, grep, cut, sort, uniq,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.160.48.68 [3/Sep/2020:18:18:10 /?C=M;O=D\n",
      "78.224.72.158 [3/Sep/2020:18:24:52 /\n",
      "192.241.199.4 [3/Sep/2020:19:11:15 /\n",
      "40.121.69.161 [3/Sep/2020:19:16:32 /.env\n",
      "40.121.69.161 [3/Sep/2020:19:16:32 /\n",
      "185.172.110.223 [3/Sep/2020:19:16:37 /\n",
      "66.249.75.16 [3/Sep/2020:19:23:28 /\n",
      "66.249.75.23 [3/Sep/2020:19:23:30 /\n"
     ]
    }
   ],
   "source": [
    "!cut -d ' ' -f1,4,7 logs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO\n"
     ]
    }
   ],
   "source": [
    "!echo \"TODO\" | grep \"DO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 2\n",
    "Avec les mêmes logs que l'exercice précédent, comptez et affichez les différents user-agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO\n"
     ]
    }
   ],
   "source": [
    "!echo \"TODO\" | grep \"DO\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
